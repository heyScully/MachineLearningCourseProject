---
title: "Machine Learning Project"
author: "Michael Scully"
date: "January 29, 2016"
output: html_document
---

# Getting Started

First, I"ll load the testing and training sets provided for the assignment

```{r}
library(caret)
training = read.csv("~/Downloads/pml-training.csv")
testing = read.csv("~/Downloads/pml-testing.csv")
```

Since the `testing` data will be used to evaluate performance of the analysis, I open to see what data will be provided in order to create a prediction.  The first 7 columns are descriptive rather than quantifications, so they will not help the prediction and can be removed.  All columns that are all `NA` in the `testing` set can also be removed from the training data, since these fields add no value in the testing data set.  Removing these extra columns will improve performance times.

```{r}

# What are the column names in testing where all values are NA?
colnames <- names(testing)[colSums(is.na(testing)) == 0]

# Remove 'problem_id' from end of list since this column does not exist in training set (and it is descriptive)
colnames <- colnames[1:59]

# Add in outcome column 'classe'
colnames <- c(colnames, 'classe')

# Subset to just these same columns in training set
training.refined <- training[,colnames]

# Further, remove descriptive fields from training set (the first 7 columns) as they add no predictive value
training.refined <- training.refined[,8:60]

```

Now that our training data is thinned out from variables that will not be useful for this assignment, I will perform `cv` (K-folds cross validation) and use the `random forest` training method.  From the lectures, random forests (rf) tend to be well suited for prediction where there are a large number of inputs with unknown interactions between them. 

```{r}
# for reproducability
set.seed(1234) 

# create train control settings  with cross validation (5 repeats instead of default 25 to decrease processing time)
control <- trainControl(method="cv", number=5, repeats=5, savePredictions = TRUE)

# train the model using random forest method (with K-fold cross validation defined in control)
modFit <- train(classe ~ ., method="rf", data=training.refined, trControl = control)

# view cv accuracy (indication of expected out-of-sample error)
plot(modFit)  #
getTrainPerf(modFit) 

```

The best model has an accuracy of `r getTrainPerf(modFit)$TrainAccuracy`. 

The OOB estimate of error rate is can be seen in the the finalModel output below:
```{r}
modFit$finalModel
```

Now that we have a good model, let's apply it to the testing data and return prediction results.

```{r}
prediction <- predict(modFit, newdata = testing)
prediction
```